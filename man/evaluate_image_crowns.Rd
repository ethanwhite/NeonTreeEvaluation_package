% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluate_image_crowns.R
\name{evaluate_image_crowns}
\alias{evaluate_image_crowns}
\title{Compute evaluation metrics for the hand-annotated images}
\usage{
evaluate_image_crowns(submission, project_boxes = FALSE, show = T,
  compute_PR = F)
}
\arguments{
\item{submission}{A five column dataframe in the order plot_name, xmin, xmax, ymin, ymax. Each row is a predicted bounding box.}

\item{project_boxes}{Logical. Should the boxes be projected into utm coordinates? This is needed if the box coordinates are given from the image origin (top left is 0,0)}

\item{show}{Logical. Plot the overlayed annotations for each plot?}

\item{compute_PR}{Logical. Should the grand summary precision and recall be computed?}
}
\value{
If compute_PR is True, a dataframe of precision-recall scores or each plot. If False, a dataframe with the intersection-over-union scores for each prediction.
}
\description{
Compute evaluation metrics for the hand-annotated images
}
\details{
The NeonTreeEvaluation benchmark contains evaluation data from 22 sites from the National Ecological Observation Network. Crowns were annotated by looking at a combination of the RGB image, a LiDAR-derived canopy height model, hyperspectral reflectance and, where available, field collected data on stem location. For evaluation of the field-collected crowns for 2 sites, see \code{evaluate_field_crowns}. The field-collected crowns were drawn by a tablet into the field, while the image crowns were drawn by looking at multiple sensor data on a screen.
This function is a wrapper for \code{evaluate_plot}. It first looks which plot_names match the benchmark dataset. Plots with no predictions, or which are not included, are ignored.
}
