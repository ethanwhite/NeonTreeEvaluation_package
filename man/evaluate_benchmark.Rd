% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluate_benchmark.R
\name{evaluate_benchmark}
\alias{evaluate_benchmark}
\title{Evaluate submission for all plots
Run evaluation on all plots in a dataframe.}
\usage{
evaluate_benchmark(submission, project_boxes = FALSE, show = T,
  compute_PR = F)
}
\arguments{
\item{submission}{A five column dataframe in the order plot_name, xmin, xmax, ymin, ymax. Each row is a predicted bounding box.}

\item{project_boxes}{Logical. Should the boxes be projected into utm coordinates? This is needed if the box coordinates are given from the image origin (top left is 0,0)}

\item{show}{Logical. Plot the overlayed annotations for each plot?}

\item{compute_PR}{Logical. Should the summarize precision and recall be computed? If not, return the IoU overlap for each ground truth box}
}
\value{
If compute_PR is True, a dataframe of precision-recall scores or each plot. If False, a dataframe with the IoU scores for each prediction for all plots.
}
\description{
Evaluate submission for all plots
Run evaluation on all plots in a dataframe.
}
\details{
This function is a wrapper for \code{evaluate_plot}. It first looks which plot_names match the benchmark dataset. Plots with no predictions, or which are not included, are ignored.
}
