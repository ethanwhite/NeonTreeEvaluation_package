---
title: "Benchmark Evaluation"
author: "Ben Weinstein"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{evaluation}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE,
  message = FALSE,
  fig.align = 'center',
  fig.height = 4,
  fig.width = 6
)
```

# Submission Format

The format of the submission is as follows

* A csv file
* 5 columns: Plot Name, xmin, ymin, xmax, ymax

Each row contains information for one predicted bounding box.

The plot column should be named the same as the files in the dataset (e.g. SJER_021) without file extensions.

```{r}
library(dplyr)
library(NeonTreeEvaluation)
library(reshape2)
library(ggplot2)
library(lidR)

#Load sample submission
data("submission")
```

```{r}
head(submission)
```

# Image-annotated Crowns Evaluation

The main data source are image-annotated crowns, in which a single observer annotated visible trees in 200 40m x 40m images from across the United States. For each plot compute the precision and recall based on intersection-over-union of 0.5 between the ground truth bounding boxes and the predicted boxes. Then loop through all matching plot_names and produce evaluation scores.

```{r,eval=FALSE}
results<-evaluate_image_crowns(submission = submission,project_boxes = T, show=F, summarize = T)
```

There are several options that need to be considered. see ?evaluate_benchmark. For example, `project_boxes` asks whether the predicted boxes should be projected in universal-transverse mercator projection (UTM) from the image coordinate system. For example,

These boxes are in the image coordinate system. The xmin of the first box is 299.53 (pixels) from the top left. The images are projected and have units in meters. Therefore we need to transform the image coordinates to the geographic coordinates for the boxes to overlap. 

For the sake of computation in this small vignette, let's select just 10 images.

```{r}
results<-submission %>% filter(plot_name %in% sample(plot_name,15)) %>% evaluate_image_crowns(.,project = T,show=T,summarize = T)
```

### Overall Performance

What is the average bounding box recall and precision across all images?

```{r}
results[["overall"]]
```

### By Site

Report the average statistics for each NEON site from across the United States. For a list of site abbreviations and locations see https://www.neonscience.org/field-sites/field-sites-map. 

```{r}
df<-melt(results[["by_site"]])
ggplot(df,aes(y=value,x=Site)) + geom_point()  + facet_wrap(~variable) + coord_flip()
```

### Plot Level Crown Counts

Number of predicted trees versus number of image-annotated trees for each plot. A good model will fall on the 1:1 line.

```{r}
df<-results[["plot_level"]]
ggplot(df,aes(y=submission,x=ground_truth)) + geom_point() + geom_abline(linetype="dashed") + coord_equal() + labs(x="Ground Truth Crowns",y="Predicted Crowns")
```

# Field-annotated crown evaluation

The second data source is a small number of field-deliniated crowns from three geographic sites. These crowns were drawn on a tablet while physically standing in the field, thereby reducing the uncertainty in crown segmentation.


# Field stem evaluation

The third data source is the NEON Woody Vegetation Structure Dataset. Each tree stem is represented by a single point. This data has been filtered to represent overstory trees visible in the remote sensing imagery.

```{r}
stem_results<-submission %>% filter(plot_name %in% sample(plot_name,10)) %>% evaluate_field_stems(.,project = T)
ggplot(stem_results,aes(x=field,y=submission)) + geom_point() + geom_abline() + coord_fixed() + ggtitle("Number of overstory trees") + labs(x="NEON Field Stems",y="Predicted Crowns")
```
